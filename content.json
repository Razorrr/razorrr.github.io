[{"title":"现代数学与数学空间","date":"2017-04-01T05:32:45.000Z","path":"2017/04/01/现代数学与数学空间/","text":"现代数学以集合为研究对象，这样的好处就是可以将很多不同问题的本质抽象出来。因为每个人的研究方向不同，为了能有效研究集合，必须给集合赋予一些“框架”（或叫做”结构“）。从数学的本质上看，最基本的集合有两种： 线性空间 度量空间 对于线性空间，主要研究集合的描述，直观的说就是告诉别人这个集合是什么样子。为了描述清楚，首先引入了基（相当于三维座标系）的概念，对于一个线性空间来说，集合中的元素只要知道其在给定基下的座标即可。但是，线性座标系中的元素没有长度的概念，为了量化线性空间中的元素，所以又在线性空间中引入了特殊的“长度”，即范数。线性空间+范数=赋范线性空间但是赋范线性空间中没有角度的概念，为了解决这个问题，又在线性空间中引入了内积的概念。因为有了度量，所以可以在度量、赋范线性空间以及内积空间中引入极限，但抽象空间中的极限和实数极限很大的一个不同点，就是极限点可能不在原先给定的集合中（即造成了原先集合少点、或者少皮），所以又引入了完备的概念，完备的内积空间即Hilbert空间。至此，我们的升级版的线性空间已包括： 基 范数 内积 完备性 线性空间 + 范数 = 赋范空间赋范空间 + 线性结构 + 内积 = 内积空间内积空间 + 完备性 = Hilbert空间","tags":[{"name":"数学","slug":"数学","permalink":"http://xrazor.net/tags/数学/"}]},{"title":"word2vec简单实践","date":"2017-03-29T05:10:00.000Z","path":"2017/03/29/word2vec简单实践/","text":"word2vec 简介 word2vec是google的一个开源工具，能够根据输入的词的集合计算出词与词之间的距离。 它将term转换成向量形式，可以把对文本内容的处理简化为向量空间中的向量运算，计算出向量空间上的相似度，来表示文本语义上的相似度。 word2vec计算的是余弦值，距离范围为0-1之间，值越大代表两个词关联度越高。 词向量：用Distributed Representation表示词，通常也被称为“Word Representation”或“Word Embedding（嵌入）”。安装Gensim 和分词工具Gensim这里主要通过Gensim模块来实现word2vec:1pip install gensim -i https://pypi.douban.com/simple 后面用豆瓣的源会快一些。 分词工具分词工具有很多：中科院NLPIR，哈工大LTP，结巴分词等。我这里用的是结巴分词。如果没有也是先用pip安装。1pip install jieba -i https://pypi.douban.com/simple 语料准备随便下载一部小说，我这里用的是&lt;天龙八部&gt;。下下来可能是乱码。可以自己写一段小代码重新加工一下:12345678910111213#!/usr/bin/env python# -*- coding: utf-8 -*-file_in = open(&apos;/home/luke/Downloads/tlbb.txt&apos;, &apos;r&apos;)file_out = open(&apos;/home/luke/Downloads/tlbb_utf8.txt&apos;, &apos;w&apos;)line = file_in.readline()while line: newline = line.decode(&apos;GB18030&apos;).encode(&apos;utf-8&apos;) #用GBK、GB2312都会出错 print newline, print &gt;&gt; file_out, newline, line = file_in.readline()file_in.close()file_out.close() 训练模型1234567891011# -*- coding: utf-8 -*-import gensim.models.word2vec as w2vmodel_file_name = &apos;tlbb_model&apos;sentences = w2v.LineSentence(&apos;/home/luke/Downloads/tlbb_segments.txt&apos;)model = w2v.Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)model.save(model_file_name)similar_list = model.most_similar(&quot;慕容复&quot;.decode(&apos;utf-8&apos;), topn=10)print model.similarity(&quot;慕容复&quot;.decode(&apos;utf-8&apos;), &quot;段誉&quot;.decode(&apos;utf-8&apos;))for word, similar in similar_list: print word, similar 好了，大功告成，现在我们来试试跑一跑，恩，结果如下：1234567891011120.922225864416萧峰 0.940936326981游坦之 0.924276173115段誉 0.922225773335乔峰 0.920364379883丁春秋 0.91599714756童姥 0.914369821548段延庆 0.912800788879鸠摩智 0.912337422371段正淳 0.911094427109邓百川 0.905085086823 慕容复和段誉的相关性是0.922，而与慕容相关性最高的是萧峰，果然是北萧峰南慕容，怎样都会被拿来相提并论。可以看出，排名前十的几个人也都是小说中与慕容复纠葛较多的人，也就是说我们的模型是有说服力的。关于word2vec，还有许多有趣的应用，以后有时间再慢慢研究。","tags":[{"name":"python","slug":"python","permalink":"http://xrazor.net/tags/python/"},{"name":"分词","slug":"分词","permalink":"http://xrazor.net/tags/分词/"},{"name":"NLP","slug":"NLP","permalink":"http://xrazor.net/tags/NLP/"},{"name":"word2vec","slug":"word2vec","permalink":"http://xrazor.net/tags/word2vec/"}]},{"title":"机器学习的几个规定动作","date":"2017-03-20T09:17:19.000Z","path":"2017/03/20/机器学习的几个规定动作/","text":"我们拿到一个机器学习项目之后，通常都会经过以下几个步骤： 1. Insight 数据洞察统计机器学习一般是从已知数据集中训练模型，并用模型对未知数据进行预测。因此对已知数据的理解变得尤为重要。 Common Sense 善于利用常识这个放在第一位，是因为刚开始做机器学习的同学，可能会觉得任何事情都从数据统计的角度去观察会很fancy。但实事是，这样的思维有时候会让我们只见树木不见森林，陷入到数据的细节里面。为了避免这种情况的发生，我们要做的就是首先通过肉眼观察数据，通过直觉来判断那几个Category有可能会在后面作为Features。以Kaggle的入门Case——Titanic为例。Data Category不多:ID, 生还情况， 船舱等级， 名字， 性别， 年龄， 家属人数， 船票信息， 票价， 仓位， 登船码头。从直觉来看，与生还情况相关性最大的，应该是性别和年龄，因为在国外Children and Lady first的道德环境，小孩和女人的生还概率可能较大；此外，从阶级等级来看，有钱人可能会得到优先安排而有较高的生还概率，而有钱人一般住高级舱，Fare也比较高， 而年少多金的人（船舱等级 X 年龄），身份可能更为显赫。另外同船家属人数的多少，可能也是一个Feature，因为一家人可能会相互帮助从而提高生还率。等等。这样我们通过常识和直觉就能得到了几个方向的猜测，至于对与不对，可以留到后面再来验证。 Data Visualization 数据可视化Python下用pandas很容易做数据统计描述，但是有时候数据不如图表那么直观，我们需要用matplotlib和seaborn等可视化工具来生成图表，帮助我们快速分析，并且在整个项目过程中，可视化是很重要的可能会被反复使用的技能，它能帮助我们保持思路清晰。在这个过程中，上一步的许多猜测会得到进一步的验证。一边做可视化，一边确定哪些是要带入模型的Feature， 同时可能还要人工设计几个Features，比如Titanic case里面的（船舱等级X年龄）。 2. Preprocessing 数据预处理缺失值填充对于年龄等数值型的数据，可以通过Randomforest拟合出一个数据，但是由于年龄这种东西，并不是船舱等级、船票等其他类的数据一定存在太大关系，因此用这样的方法，虽然看起来fancy，但是并不一定有太大作用，可能直觉乱填一个效果都比之要好，因此也不能时刻迷信算法的结果。其他的思路还有取区间平均值，或者中位数。这都需要大量的试才能得到好的结果。并不是一蹴而就的事情。 0-1化有些数据类别本身不是数值型的，而是一些字符型的类别，为了便于带入模型计算，我们要把它转化为0-1数据，也就是Dummy Code。这一步可以直接用pandas里面的get_dummies方法，实现很方便。 归一化当数据与数据之间的范围相差很大，比如说年龄的范围从1岁到80岁，而船舱的等级就是1-3级，这样会导致带入模型使用梯度下降法或者拟牛顿法求解时，数据收敛很慢，甚至是不收敛。因此要对数据做scaling，就是归一化， 把数据归一到一个很小的范围，比如（-1，1）.可以调用sklearn的preprocessing处理。 数据弃用代入模型的时候肯定不是所有数据都代入，选择好Features之后把不用的数据类别都drop掉。通过以上的步骤我们得到了一组clean data，下一步就是构建模型了。但是不要看前面这两步简单，其实有很多判断和取舍，并且需要反复的测试对比，才能得到最终的Features，所以俗话说得好：数据和特征决定了机器学习的上限，而模型和算法只是不断的逼近这个上限。很多项目的大部分时间（&gt;70%）实际上都花在特征工程上，可见其重要性。 Modeling 建模选择方法在做kaggle的时候，大家可以很方便的调用sklearn的各种机器学习方法，比如LR,SVM,NB,DT,KNN等等，反正就几行代码的事。事实上，我猜测，在工业界，实际选择方法的时候一是考虑模型的泛化能力，二是考虑模型的鲁棒性， 三是考虑模型的计算消耗， 四是考虑模型的解释性。因此并不像我们“游戏”里面选择那么自由。在kaggle中我们可以根据项目要解决的问题，选择相对匹配的算法，比如项目要解决的是二分类问题，那么可能LR,SVM,DT是相对好的选择；如果项目是图像或语音识别，那可能CNN等深度学习算法优势会更大。等等。 模型评估Optimization 性能优化基于数据的优化基于算法的优化基于参数的优化基于模型融合的优化","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://xrazor.net/tags/机器学习/"}]},{"title":"OPEN分析框架","date":"2017-03-17T05:19:09.000Z","path":"2017/03/17/OPEN分析框架/","text":"最近对自己的分析企业、项目运营的思维框架做了一下梳理，可归纳为OPEN框架。 对企业的OPEN框架Organization 组织及人力资源结构 层级设置（） 管理界面(管事)管理界面越清晰，公司的事务越好处理；管理界面越复杂，越容易出现扯皮拉筋的事。 晋升制度、薪酬体系、淘汰通道 权利距离 Process 业务流程 供 产 销 研Environment 企业环境 政策环境 行业环境Net Profit 净利润 收入 成本对项目运营的OPEN框架Objective 优化目标函数建立一个可充分描述的唯一的优化目标函数。Process 业务漏斗根据业务场景建立业务漏斗，如“暴光——点击——理解——认同——留存——转化” 或 “浏览———收藏——加购物车——下单”等。Experiment 实验测试基于业务数据对比不同周期每个漏斗环节数据差异，找出影响目标函数的具体环节及变量。比如做ABtest验证预判是否正确。Net 目标分解与业务联接实际上这一步应该在P前面，但是为了凑这个OPEN故挪到了最后。这个环节做的事就是将目标函数分解为具体可量化的业务数据。如“eCPM = 点击数 X 点击价值= 暴光率 X 点击率 X 点击价值 =…”然后将量化指标分别与业务转化漏斗中的具体环节对应。","tags":[{"name":"分析","slug":"分析","permalink":"http://xrazor.net/tags/分析/"},{"name":"框架","slug":"框架","permalink":"http://xrazor.net/tags/框架/"}]},{"title":"机器学习中比算法更重要的——凸优化(1)","date":"2017-03-02T02:45:07.000Z","path":"2017/03/02/机器学习中比算法更重要的——凸优化/","text":"在机器学习过程中，最初认为算法是最核心的问题，但是随着学习的推进，慢慢觉得Optimization是更加关键的知识，所以将学习心得整理于此。关于凸优化的问题主要学习两本书， 一本是,这本比较偏工程，值得好好研习，另一本是，这本比较偏理论。 基础概要什么是建模?对特定问题界定目标函数、变量、约束条件的过程，就叫建模。第一步也是最重要的一步，就是针对问题找到一个Objective，即目标函数。如果模型过于简单，则没有办法很好的描述实际问题；如果模型过于复杂，也会难以解决。 什么是优化算法？当模型搭建起来后，为使模型能够获得最优解的过程就是优化，而使用的算法就叫优化算法。事实上并没有普适的优化算法， 针对不同的问题，优化算法各不相同。所选择的优化算法是否合适，直接关系到计算过程是否迅速，以及——是否能找到问题的解。 全局优化、局部优化与凸函数局部优化速度明显高于全局优化，因此在实际工程中，我们总是将全局优化的问题尽量转化为局部优化的问题。而在Convex Programme中，所有的局部最优解等于全局最优解，这就是我们要研究凸优化出发点和意义所在。 机器学习为什么要掌握优化的知识？一方面是为了求得模型的最优解；另一方面是基于算法工程师的工作特性——不能通过多次测试程序的方式，验证所求解是否正确或者这个算法根本上是否work，只能通过数学上的方法来检验（如Sensitive Analysis敏感性分析）。因此Optimization是机器学习必修的一项技能。 凸集与凸函数 S ∈ IR n is a convex set if the straight line segment connecting any two points in S lies entirely inside S. 集合中任意两点的连线，都落在来这个集合内，那么这个集合就是凸集。而凸函数是什么？一张图解释： 函数（蓝色）是凸的:当且仅当其上方的区域（绿色）是一个凸集。而一个无约束函数如果是凸函数，那么通常会收敛到一个固定值（最大值、最小值、拐点），换句话说，凸函数是一定可解的。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://xrazor.net/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"http://xrazor.net/tags/算法/"},{"name":"凸优化","slug":"凸优化","permalink":"http://xrazor.net/tags/凸优化/"}]},{"title":"用Scrapy爬取51job数据挖掘岗位进行数据分析（1）","date":"2017-02-12T06:44:57.000Z","path":"2017/02/12/用Scrapy爬取51job数据挖掘岗位进行数据分析/","text":"近期在找数据挖掘的相关工作，所以想看一看在职位描述中所提及的技能树主要有哪些，以指导自己学习完善。遂用Python的Scrapy框架做了一只爬虫，爬取了数据挖掘相关的70770条岗位信息，并对其进行了简单的分析。本篇只写几个简单的结论，实现部分下一篇再写。本次用到的主要工具和模块如下：1234567OS:ArchlinuxPython 2.7 Scrapy -- 爬虫框架MongoDB -- 用以存储爬下来的数据Jieba -- 分词Pandas -- 数据计算分析Pygal -- 生成图像 数据挖掘主要用到的技能树这块主要用Jieba对爬下来的职位描述内容进行分词，并根据词频生成标签云： 从图中可以看出，Python凭借其良好的语言特性在数据挖掘领域也得到广泛应用，而JAVA则紧随其后；另外数据库应用也是数据挖掘的必备技能，其中传统的关系型数据库如MySQL，Oracle等应仍属于必修领域，反而非关系型数据库如MongoDB等可能由于市场还没有广泛使用而导致大家还没有普遍应用。统计分析工具SPSS也频频出镜。而作为数据存储和数据分析之间的必备技能ETL也自然出现在各职位需求中。在大规模数据存储方面SPARK和HADOOP,以及HIVE、HBase等技能也称为各家的“家常菜”。另外可以看到LINUX应该是主要的工作系统，因此在职位描述中提到的频率也很高。此外，还有最基础的EXCEL,PPT等，仍作为配套需求技能较普遍出现在职位要求中。 数据挖掘薪酬分布 如图所示，数据挖掘由于学科跨度大、学习曲线陡峭，因此市场给出的价格普遍不低， 普遍在1万往上走。 数据挖掘职位地区分布 北上广深还是数据挖掘职位的主要需求区域，另外，杭州和郑州需求也不可小觑，说明这两地互联网氛围日趋浓厚。 数据挖掘学位和薪水的关系 在1-2万薪水这个区间，主要为本科学历， 而在4.5-8千的区间，大专学历占比要比本科学历大。另外值得注意的是，很多企业在学历栏的要求是缺省的，也可以从一定程度说明，在数据挖掘领域，只要有真本领，能解决问题，不管学历高低，仍是可以在市场上找到立足之地的，较符合“YOU CAN YOU UP”的逻辑。 在需求前十区域数据挖掘工作经验和薪水的关系 在这里更是凸显互联网行业“YOU CAN YOU UP”的逻辑， 有较多企业在工作经验要求上缺省，也有可能是在工作要求里作了另外说明（本文工作经验爬取信息的来源是系统后台有生成的标注），因时间关系此处没有作另外处理。由图中还可以看出，工作两年和工作3-4年也是一个抢手的区间。","tags":[{"name":"python","slug":"python","permalink":"http://xrazor.net/tags/python/"},{"name":"scrapy","slug":"scrapy","permalink":"http://xrazor.net/tags/scrapy/"},{"name":"数据分析","slug":"数据分析","permalink":"http://xrazor.net/tags/数据分析/"}]},{"title":"计算机字符编码手札：ASCii、Unicode和UTF-8","date":"2017-02-10T22:37:53.000Z","path":"2017/02/11/计算机字符编码手札：ASCii、Unicode和UTF-8/","text":"基础知识首先，计算机编码的结构为: 二进制位（Bit）——二进制字符串——字节（8 Bit） 众所周知，二进制是计算机信息表达的基础。每个Bit有0和1两种状态，8个二进制位就可以有256种状态，称为1个字节。 ASCii编码有了上面的设定，人们就想了——怎么把日常的信息转化位计算机的执行语言呢？即建立计算机语言和人类语言的桥梁。于是ASCii出现了，它规定了128个二进制字符串，使之与英文字母等常用字符一一对应。比如大写字母A对应的二进制字符串是01000001。这128个符号（包括32个不能打印出来的控制符号），只占用了一个字节的后面7位，最前面的1位统一规定为0。 Unicode慢慢的，不光是英文需要与计算机语言桥接，法语日语中文俄文也要啊！所以你搞一个编码我搞一个编码，纷纷把自己的语言和计算机语言联系起来。结果悲剧了，不同的编码方式导致了解码的时候得到的信息不一致。法文编码的用日文的解码方式得出来一堆乱码。怎么办？为解决这个问题，Unicode应运而生。它包含了世界上所有文字符号，每个符号都有一个独一无二的编码。这样就不会出现乱码的问题了。它是类似这样的：&amp;#x0041（表示大写字母A）。但是，Unicode并不是没有问题。它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储。比如，汉字”严”的unicode是十六进制数4E25，转换成二进制数足足有15位（100111000100101），也就是说这个符号的表示至少需要2个字节。表示其他更大的符号，可能需要3个字节或者4个字节，甚至更多。这样计算机就迷茫了。如何才能区别Unicode和ASCII？计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？另一个问题是，我们已经知道，英文字母只用一个字节表示就够了，如果Unicode统一规定，每个符号用四个字节表示，那么每个英文字母前都必然有三个字节是0，这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。 UTF-8因此大家需要一个通用的Unicode的实现方式，UTF-8就此出现。它的前提思想就是用第一个字节来告诉计算机——“我是多少字符表示的符号“。因此它是一种可变长的编码方式，字符可以用单字符（英文）也可用双字符（中文）甚至更多的字符（更大的符号）来表示。它的规则如下： 1）对于单字节的符号，字节的第一位设为0，后面7位为这个符号的unicode码。因此对于英语字母，UTF-8编码和ASCII码是相同的。 2）对于n字节的符号（n&gt;1），第一个字节的前n位都设为1，第n+1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的unicode码。下表总结了编码规则，字母x表示可用编码的位。1234567Unicode符号范围 | UTF-8编码方式(十六进制) | （二进制）--------------------+---------------------------------------------0000 0000-0000 007F | 0xxxxxxx0000 0080-0000 07FF | 110xxxxx 10xxxxxx0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 跟据上表，解读UTF-8编码非常简单。如果一个字节的第一位是0，则这个字节单独就是一个字符；如果第一位是1，则连续有多少个1，就表示当前字符占用多少个字节。下面，还是以汉字”严”为例，演示如何实现UTF-8编码。已知”严”的unicode是4E25（100111000100101），根据上表，可以发现4E25处在第三行的范围内（0000 0800-0000 FFFF），因此”严”的UTF-8编码需要三个字节，即格式是”1110xxxx 10xxxxxx 10xxxxxx”。然后，从”严”的最后一个二进制位开始，依次从后向前填入格式中的x，多出的位补0。这样就得到了，”严”的UTF-8编码是”11100100 10111000 10100101”。 Python中编码的处理首先就要弄清础，你想搞的对象是啥？是str？还是Unicode，这样你才能知道是用encode还是decode。123&gt;&gt;&gt; isinstance(&apos;object&apos;,str)&gt;&gt;&gt; instance(&apos;object&apos;,unicode) 不要对str使用encode，不要对unicode使用decode。","tags":[{"name":"python","slug":"python","permalink":"http://xrazor.net/tags/python/"},{"name":"encoding","slug":"encoding","permalink":"http://xrazor.net/tags/encoding/"}]},{"title":"Live-a-boxer-life","date":"2017-02-10T21:41:07.000Z","path":"2017/02/11/Live-a-boxer-life/","text":"早晨5:25分。 我又在挣扎是否起床，外面天寒地冻，内心又在OS“五点钟起床的人生才有希望”。 不知道为什么，随着年纪渐长，反而起床要靠前一夜打的鸡血来帮助。比如时不时跳出来刷屏的科比曰——”我知道每天凌晨洛杉矶四点的样子“，还有XX董事长作息时间表——清一色的五点起床。 这些确实给了我们激励。 但是，回想小时候一年级到初三九年田径队的生活，发现小时候的自己就已经能做到。无论严寒还是酷暑，每天五点半准时到达训练场地，热身压腿，50米高抬腿，车轮跑，起跑练习，60米往返跑，100米竞速跑，等等。每天来一套。汗流浃背之后再赶到教室上课。种子选手在放学前一个小时还有一个特权——提前下课到田径场训练——直至傍晚6点。周而复始。 苦吗？并不觉得。 最高兴的是市运会比赛归来（大家还是上课途中），老师问到“跑来第几呀？”然后能淡淡说上一句——“第一。“ 心里的那种得色，也是没谁了。但是不幸如果哪一次没拿第一，感觉羞愧得没办法开口回答，总觉着愧对了提前下课的特权。 实际名次多少当时并没有想得太多，能支撑这么长久的坚持的，是享受竞速的感觉，享受”准备超越“到”超越“的过程，无论是超越自己还是超越对手。这样的过程能保持人生的一种竞技状态——自律、刻苦、追求卓越。 要求自己早起的人，也无非是想要找到这种竞技状态，像运动员一样超越，像拳击手一样去打到眼前的困难。 因此，不要人生多灿烂，只要一直在路上，管他山高路远，管他虎豹豺狼，就是要攀登要跨越。 哎呀，被小时候的自己打了个鸡血，学习去了。 写于2017年元宵。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://xrazor.net/tags/随笔/"}]},{"title":"MapReduce是小学生都会玩的小把戏","date":"2017-02-09T06:28:29.000Z","path":"2017/02/09/Mapreduce是小学生都会玩的小把戏/","text":"MapReduce是啥，其实思想很简单，我们小学生问题活动时经常会做，比如，我们要清点图书馆里的书总共有多少本，那么会有如下安排: 小明数1号书架，大波数2号书架…Map的过程就是分配的过程 把每个人数了多少本书加总…Reduce的过程就是汇总的过程 显然，人越多数得越快，这就是MapReduce。 （完）","tags":[{"name":"mapreduce","slug":"mapreduce","permalink":"http://xrazor.net/tags/mapreduce/"},{"name":"大数据","slug":"大数据","permalink":"http://xrazor.net/tags/大数据/"},{"name":"hadoop","slug":"hadoop","permalink":"http://xrazor.net/tags/hadoop/"}]},{"title":"在archlinux上安装mongodb","date":"2017-02-09T02:51:40.000Z","path":"2017/02/09/在archlinux上安装mongodb/","text":"用pacman命令安装mongodb1$ sudo pacman -S mongodb 用whereis命令找到mongo的执行文件夹1$ whereis mongo 一般是/usr/bin 创建一个数据存储文件夹选择一个路径，比如/data/db，使用mkdir命令创建：1$ sudo mkdir /data/db 顺便把文件夹的权限改为可读写，否则数据文件无法写入，mongodb也没办法顺利启动：1$ sudo chmod 777 /data/db 顺带提一下，几种文件权限的数字代码： -rw——- (600) 只有所有者才有读和写的权限 -rw-r–r– (644) 只有所有者才有读和写的权限，组群和其他人只有读的权限 -rwx—— (700) 只有所有者才有读，写，执行的权限 -rwxr-xr-x (755) 只有所有者才有读，写，执行的权限，组群和其他人只有读和执行的权限 -rwx–x–x (711) 只有所有者才有读，写，执行的权限，组群和其他人只有执行的权限 -rw-rw-rw- (666) 每个人都有读写的权限 -rwxrwxrwx (777) 每个人都有读写和执行的权限 创建config文件指定mongodb的数据写入路径cd到/usr/bin里面（当然你也可以换另一个路径）1sudo nano mongodb.config 然后在文件中写一行:1dbpath=/data/db 作用就是在启动时告诉mongod说我们的数据是要存在/data/db这个地方。还可以直接用命令行指定（如果不担心自己忘记路径的话）：1$ mongod --dbpath &quot;/data/db&quot; 带参数启动mongodcd到/usr/bin里面1$ mongod --config mongodb.config 这样就能启动mongod了，需要注意的是注意区分mongod和mongo，mongod是service processing，而mongo是client shell。当你看到这一句时说明已经成功启动：12...2017-02-09T11:09:45.613+0800 I NETWORK [thread1] waiting for connections on port 27017 启动mongo这时启动另一个命令行终端，然后启动mongo，OK了：1$ mongo","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://xrazor.net/tags/mongodb/"},{"name":"archlinux","slug":"archlinux","permalink":"http://xrazor.net/tags/archlinux/"}]},{"title":"从造物者的角度理解python类中init的意义","date":"2017-02-08T07:35:19.000Z","path":"2017/02/08/从造物者的角度理解python类中init的意义/","text":"怎么理解python在创建一个class时，init 的意义呢？我要说的是一个比喻。可以把程序员比喻为造物者，是一个神，有一天他想创造一个东西， 叫人类，所以：1class human 他想啊想，觉得人类应该有鼻子眼睛有手有腿，所以：1234567class human def __init__(self,nose,eye,hand,leg): self.nose = nose self.eye = eye self.hand = hand self.leg =leg 这样人类的这些类属性就定下来了。到这里还是神心中的一些想法，还没有真正造人。等到了星期一这一天的黄道吉日，神决定把人造出来（实例化）。他说，要有人…于是出来了一堆人，这些人的共同特征就是有鼻子眼睛有手有腿，但是他们的鼻子眼睛手脚都各不相同。所以到这里我们就可以稍微理解一些了，这个init是干什么的？它就是设置这一类里面所有的参数的。再想想 键-值 的关系？ init做的就是创建 键 ，而被其他方法调用就是在 写这个键的值。OVER","tags":[{"name":"python","slug":"python","permalink":"http://xrazor.net/tags/python/"}]},{"title":"Dataframe常用操作","date":"2017-02-08T06:31:44.000Z","path":"2017/02/08/Dataframe常用操作/","text":"比如有一个数据集，将其转化为Dataframe格式：1frame = Dataframe(datas) 之后可对frame作如下操作： 统计描述frame.describe()查看数据摘要，包括数据查看（默认共输出首尾60行数据）和行列统计，包括计数，均值，std，各个分位数。由于源数据通常包含一些空值甚至空列，会影响数据分析的时间和效率，在预览了数据摘要后，需要对这些无效数据进行处理。 frame.mean()默认对每一列的数据求平均值；若加上参数a.mean(1)则对每一行求平均值。 frame.apply(lambda x:x.max()-x.min())表示返回所有列中最大值-最小值的差。 缺失值处理frame.isnull()查看数据表中哪些为空值，与它相反的方法DataFrame.notnull() ，Pandas会将表中所有数据进行null计算，以True/False作为结果进行填充。 frame.dropna()不加参数的情况下， dropna() 会移除所有包含空值的行。如果只想移除全部为空值的列，需要加上 axis 和 how 两个参数。 frame.replace()可用来处理剩余行冗余数据，使之合规并压缩数据存储空间。 frame.fillna(value=x)表示用值为x的数来对缺失值进行填充。 数据类型查看frame.dtypes查看每列的数据类型，Pandas默认可以读出int和float64，其它的都处理为object，需要转换格式的一般为日期时间。 frame.astype()对整个DataFrame或某一列进行数据格式转换，支持Python和NumPy的数据类型。 frame.sort_index(axis=1,ascending=False)axis=1表示对所有的columns进行排序，下面的数也跟着发生移动。后面的ascending=False表示按降序排列，参数缺失时默认升序。","tags":[{"name":"python","slug":"python","permalink":"http://xrazor.net/tags/python/"},{"name":"pandas","slug":"pandas","permalink":"http://xrazor.net/tags/pandas/"}]},{"title":"信息熵、信息增益与Gini指数","date":"2017-02-03T07:51:28.000Z","path":"2017/02/03/信息熵、信息增益与Gini系数/","text":"在机器学习决策树中，引入里熵、信息增益、信息增益比和Gini指数，作为特征选择的准则。不同的算法对应了不同的特征选择准则，如ID3选用信息增益的大小作为特征选择判定的标准，C4.5则选用信息增益比为选择标准，CART则用Gini指数作为选择标准。那么，所引入的这几个指标的内涵意义是什么呢？本文将作一个简单的梳理。 信息熵在信息论里面，熵被用来衡量一个随机变量出现的期望值，对不确定性的测量，是描述一个系统所需的最低存储单元。在信息世界，熵越高，所需传递的信息越丰富，熵越低，则意味着所需传输的信息越少。而在热力学里，熵越高，表示混乱度越高。这三个解释乍一看毫无关联，实际上确实可以相互印证的。以抛硬币为例，自然条件下结果为正面或反面的概率均为50%， 此时结果的不确定性最高， 越混乱，而我们要描述这个结果所需的文字（暂且用文字的个数代表存储单元）越多，比如你会这么描述“结果可能是正面，也可能是反面”（加逗号14个字）；而当我们假定正面出现的概率为100%时，信息熵为0，不混乱很清晰，而此时我们描述这个结果所需的存储单元是很少的，比如说“是正面”（3个字）就能描述清楚。 信息增益有了上面的概念，我们就可以结合ID3算法来看看信息增益是什么东西。在ID3中，使用信息增益作为特征选择的准则，用H（D）表示数据集的经验熵，用H（D|A)表示在A特征下数据集D的经验条件熵，则信息增益g（D，A）= H(D) - H(D|A), 表示 有A 和 没A ，整个系统的不确定性变化了多少。这里要牢记， 熵高，是说明不确定很大，如果H（D）很大，而 H（D|A)很小，则g（D,A）会趋大， 说明特征A 的存在，大大降低了系统的不确定性， 因此它是一个关键的特征。 Gini指数Gini指数是CART算法中生成分类树时用来选择特征的一个指标。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://xrazor.net/tags/机器学习/"}]},{"title":"P问题、NP问题与NPC问题","date":"2017-02-02T09:11:11.000Z","path":"2017/02/02/P问题、NP问题与NPC问题/","text":"时间复杂度要想分清这三个问题，首先需要了解时间复杂度的概念。时间复杂度表示了当一个问题的规模扩大后，程序需要的时间长度增长得有多快。是一个比较量。当数据变大后，数据处理时间是不变，还是同比例变长，还是阶乘级别的变长？这是一个用来衡量计算机算法的重要考量。因此，我们按以下划分并定义时间复杂度： O(1)：常数级时间复杂度。无论数据规模怎么变化，计算机处理时间不变。 O(n)：线性时间复杂度。数据规模增长多大，计算机处理时间就增长多长。 O(n^2)：冒泡时间复杂度。 O(n！)：阶乘时间。随着数据规模增长，处理时间按阶乘级增长。…以此类推，还有幂对数时间复杂度、指数时间复杂度等不同的时间复杂度。 容易看出，前面的几类复杂度被分为两种级别，其中后者的复杂度无论如何都远远大于前者：一种是O(1),O(log(n)),O(n^a)等，我们把它叫做多项式级的复杂度时间，因为它的规模n出现在底数的位置；另一种是O(a^n)和O(n!)型复杂度，它们是非多项式级的复杂度时间，其复杂度计算机往往不能承受。当我们在解决一个问题时，我们选择的算法通常都需要是多项式级的复杂度，非多项式级的复杂度需要的时间太多，往往会超时，除非是数据规模非常小。 P问题如果在一个问题可以在多项式时间里找到可以解决它的算法，那么这个问题就是P问题。 NP问题NP问题是可以在多项式时间里验证一个解的问题。 NPC问题约化 Reducibility有A、B两个问题，如果A 能用 B 的算法解决，那么称 A 可以约化为 B，并且认定 B 问题比 A 问题复杂。约化具有传递性，即：A 可以约化 B， B可以约化为 C，则A 可以约化为C。当然，我们所说的“可约化”是指的可“多项式地”约化(Polynomial-time Reducible)，即变换输入的方法是能在多项式的时间里完成的。约化的过程只有用多项式的时间完成才有意义。从约化的定义中我们看到，一个问题约化为另一个问题，时间复杂度增加了，问题的应用范围也增大了。通过对某些问题的不断约化，我们能够不断寻找复杂度更高，但应用范围更广的算法来代替复杂度虽然低，但只能用于很小的一类问题的算法。 NPC到这里NPC就可以简单定义为：a）它是一个NP问题；b）所有的NP问题到可以约化到它。满足这两个条件就是NPC问题，即NP完全问题。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://xrazor.net/tags/机器学习/"}]},{"title":"什么是范数？","date":"2017-01-24T02:49:11.000Z","path":"2017/01/24/什么是范数？/","text":"基本概念范数，是具有“长度”概念的函数。比如，二维欧几里德空间里面，用一个箭头表示矢量，对应范数就是这矢量的长度。一维空间上它是绝对值，二维空间上它是模。是不是没明白？ 要想搞懂这个，需要一些其他的知识。 补给知识1. 函数与几何的关系函数是几何的抽象概括，几何是函数的具体描述。但是，当函数超过三维时，就难以获得较好的想象。于是需要引入映射的概念。 2. 映射与矩阵的关系之前讲到三维以上几何就不好想象了，但是我们仍要对三维以上的事物做比较、做分析。怎么办？映射。通过映射，给空间内的任意一个元素配一个实数，实数就可以拿来比较了。为了更好的表达映射，我们通过矩阵来表示映射关系。因为这个元素，可以是一个数，也可以是一个集合，也可以是一个向量，所要用矩阵来表达更为方便。到这里，就可以给范数一个更具象的描述：范数就是这个集合 或 矩阵 的大小，范数大，意味着矩阵就大。其他情况也可以类推。 几种范数的意义 L0范数：权值向量w中不为0的分量的个数。是假设参数服从拉普拉斯分布（双指数分布），保证权值向量的稀疏性。 L1范数：权值向量中每个分量的绝对值之和。也是保证权值向量的稀疏性。L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。 L2范数：权值向量中各分量求平方和之后开方，即通常意义上的模。是假设参数服从高斯分布，防止过拟合。和L1分量不同，它不会让权值等于0，而是尽可能接近于0，这里面有很大的区别。L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。L2还有助于处理 condition number不好的情况下矩阵求逆很困难的问题，加速求解。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://xrazor.net/tags/机器学习/"},{"name":"范数","slug":"范数","permalink":"http://xrazor.net/tags/范数/"}]},{"title":"机器学习方法=模型+策略+算法","date":"2017-01-23T03:56:34.000Z","path":"2017/01/23/机器学习方法-模型-策略-算法/","text":"机器学习方法都是由模型、策略、算法构成的，可简单表示为1方法 = 模型 + 策略 + 算法 模型机器学习首先考虑的是要学习什么样的模型，比如说：在监督学习过程中，模型就是所要学习的 条件概率分布 或 决策函数。即：1模型 = 条件概率分布 | 决策函数 而模型的假设空间，则是包含了所有 分布 或 函数 的集合。通常假设空间里的 分布 或 函数 有无穷多个。 策略策略，就是按照什么方法去选择最优的模型。比如说监督学习里的 经验风险最小化策略、结构风险最小化策略。哈？这两个又是什么鬼？要解释这个首先得了解三个概念：损失函数（Loss Function），期望损失（Expected Loss），经验损失（Empirical Loss）。损失函数：与损失函数相关的有3个值：输入值X，输出值f（X），真实值Y。通过度量f（X）与Y之间的误差，来评判模型的好坏。误差越小，模型越好。机器学习常用的损失函数有以下几种： 0-1损失函数 平方损失函数 绝对损失函数 对数损失函数（对数似然损失函数） 期望损失： R = L（X，f（X）） P（X, f（X））期望损失是模型f（X）关于联合分布P（X,Y）的平均意义下的损失。*而我们的目标就是找到一个期望损失最小的模型。但是，P（X,Y）是未知的，因此从这个方程去求期望损失是不行的。 经验损失： R = 1/N * sum（L（X，f（X））经验损失是训练样本集的平均损失。 之前说到P（X,Y）未知因此期望损失不可求，但是，根据大数定律， 当样本容量N趋于无穷大时，经验损失趋于期望损失。这就有希望了？！但但是，现实中的训练样本数量很有限， 甚至很小，所以用经验损失来估计期望损失往往很不理想。 怎么办？这时候就是我们的策略登场的时候： 比如 经验风险最小化策略、结构风险最小化策略。 经验风险最小化策略：简单粗暴的认为经验损失最小的模型就是最好的。当样本足够多的时候，效果OK，但是样本不足即容易过拟合（Over fitting）。结构风险最小化策略：认为结构风险最小的模型最好。结构风险的函数是在经验风险的基础上多加里正则化项或罚项λJ（f），J（f）表示模型的复杂度，是假设空间上的泛函。到这里可以看出，我们以监督学习为例，到这里监督学习的问题已经变为里 经验 或 结构 风险函数 的优化问题。 算法算法，就是用什么计算方法求解最优模型，怎么在找到最优解的同时，保证求解过程的高效，是算法部分要解决的问题。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://xrazor.net/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"http://xrazor.net/tags/算法/"}]},{"title":"python中\"if __name__ == '__main__':\" 的作用","date":"2017-01-22T03:14:16.000Z","path":"2017/01/22/python中if __name__ == '__main__': 的作用/","text":"先给出一个代码的样例，譬如你有一个文档叫test.py： 1234567def test111(): print &quot;正在执行111号模块&quot; def test222(): print &quot;正在执行222号模块&quot; if __name__ == &apos;__main__&apos;: test111() test222() 因为&quot;__name__&quot;是python的一个内置函数，当程序完整执行时，python会把 __name__ = __main__； 当程序部分执行时（即调用其中的一个def），__name__= 模块名字。 两种不同的情况执行结果如下： 当直接执行test.py 的时候，__name__ = __main__，此时输出结果为： 12正在执行111号模块正在执行222号模块 当只调用test111（）时，__name__ = test111（），此时输出结果为： 1正在执行111号模块","tags":[{"name":"python","slug":"python","permalink":"http://xrazor.net/tags/python/"}]},{"title":"scrapy中对unicode的处理","date":"2017-01-15T03:37:08.000Z","path":"2017/01/15/scrapy中对unicode的处理/","text":"用scrapy爬虫框架爬取的数据，在调用extract()后， 默认数据返回的编码格式是unicode， 比如中文就会显示为\\xe7\\x94等等。为了使之在存储中显示为中文，可在pipeline.py中处理如下： 12345678class SeekjobPipeline(object): def __init__(self): self.file = codecs.open(&apos;../results.json&apos;, &apos;wb&apos;, encoding=&apos;utf-8&apos;) def process_item(self, item, spider): line = json.dumps(dict(item)) + &apos;\\n&apos; self.file.write(line.decode(&quot;unicode_escape&quot;)) return item 其中的process_item方法是必须调用的用来处理item，并且返回值必须为Item类的对象，或者是抛出DropItem异常。并且上述方法将得到的item实现解码，以便正常显示中文，最终保存到创建的results.json文件中。而json.dumps是用来将Python中（字典，列表等）变量格式化成json字符串输出。 转中文的关键点是这一句:’line.decode(“unicode_escape”)’ 注意：在编写完pipeline后，为了能够启动它，必须将其加入到ITEM_PIPLINES配置中，即在settings.py中加入下面一句：123ITEM_PIPELINES = &#123; &apos;seekjob.pipelines.SeekjobPipeline&apos;: 300,&#125;","tags":[{"name":"python","slug":"python","permalink":"http://xrazor.net/tags/python/"},{"name":"scrapy","slug":"scrapy","permalink":"http://xrazor.net/tags/scrapy/"}]},{"title":"用urllib.quote进行URL编码","date":"2017-01-12T04:24:07.000Z","path":"2017/01/12/用urllib-quote进行URL编码/","text":"python的url编码函数是在类urllib库中，使用方法是： urllib.quote(string[, safe])，除了三个符号“_.-”外，将所有符号编码，后面的参数safe是不编码的字符. 简单的例子如下： &gt;&gt;&gt; import urllib &gt;&gt;&gt; urllib.quote(&quot;a-b-c&quot;) &apos;a-b-c&apos; &gt;&gt;&gt; urllib.quote(&quot;a+b+c&quot;) &apos;a%2Bb%2Bc&apos; &gt;&gt;&gt; urllib.quote(&quot;http://test.com/a+b+c&quot;) &apos;http%3A//test.com/a%2Bb%2Bc&apos; &gt;&gt;&gt; urllib.quote(&quot;http://test.com/a+b+c&quot;, &quot;:/&quot;) &apos;http://test.com/a%2Bb%2Bc&apos; &gt;&gt;&gt; urllib.quote(&quot;http://test.com/?q=a+b+c&quot;, &quot;:?=/&quot;) &apos;http://test.com/?q=a%2Bb%2Bc&apos; 所以urllib.quote的作用是使生成的更符合规范的URL","tags":[{"name":"python","slug":"python","permalink":"http://xrazor.net/tags/python/"}]},{"title":"pip换源方法","date":"2017-01-11T02:04:58.000Z","path":"2017/01/11/关于pip换源方法/","text":"用自带的pip源简直是龟速，换成国内的镜像就快很多，比如豆瓣的镜像：http://pypi.douban.com速度就非常的快 临时换源方法pip install pythonModuleName -i https://pypi.douban.com/simplepythonMudleName直接替换为你想要下载的包的名字即可 永久换源方法我是archlinux系统，需要修改的文件是/root/.pip/pip.conf12[global]index-url = https://pypi.douban.com/simple 这样改好后就相当于更改了pip的默认下载源","tags":[{"name":"python","slug":"python","permalink":"http://xrazor.net/tags/python/"}]},{"title":"在Archlinux上安装hp1020打印机的方法","date":"2017-01-10T04:37:02.000Z","path":"2017/01/10/在Archlinux上安装hp1020打印机的方法/","text":"从windows换到linux，在打印机的问题上折腾了不少时间，网上各种教程不少，也是过很多，比如用hplip的，hplip+cups的，最后我是用foo2zjs成功安装了我的hp1020.当然其他型号的也可以依葫芦画瓢。下面开始。 下载开源驱动foo2zjs从这里进入foo2zjs页面。里面有详细的英文指南。英语不好的同鞋请跟着我的步骤走。在命令行窗口输入以下命令下载foo2zjs套件：$ wget -O foo2zjs.tar.gz http://foo2zjs.rkkda.com/foo2zjs.tar.gz你应该知道代码是从“$”符号后开始的：D 下载好后用以下命令解压并安装$ tar zxf foo2zjs.tar.gz 用cd命令，进入解压好的目录$ cd foo2zjs 用make命令编译$ make 获取针对某一型号打印机的一些额外文件$ ./getweb xxxx这里的xxxx对应你的打印机型号，比如我是hp1020，命令行则为：$ ./getweb 1020 然后正式安装驱动先获取root权限$ su然后再输入命令：$make install 安装CUPSCUPS 是一个打印机管理套件。安装驱动做的是为了让计算机能够识别到你的打印机，而安装CUPS 是为了让计算机和打印机建立联系。在UBUNTU16.04及以上的系统中输入以下命令获取CUPS：$ sudo apt install cups在Archlinux系统中用以下命令获取CUPS：$ sudo pacman -S cups安装好后用任意浏览器访问地址：http://localhost:631然后你就会看到一个管理界面，点击上方菜单你栏的Administrator，然后用root登录，输入密码，随后点Add printer。正常情况下到这一步你应该能看到打印机的具体型号，比如说HP1020XXXXX，然后跟着导航走就可以了。以上就是我的安装方法。供参考。","tags":[{"name":"Linux","slug":"Linux","permalink":"http://xrazor.net/tags/Linux/"}]},{"title":"为什么要输出自己的知识","date":"2017-01-10T01:13:10.000Z","path":"2017/01/10/为什么要输出自己的知识/","text":"为什么要习惯“输出”知识？我们每个人都经历了十几年甚至几十年漫长的学习生涯， 不知道大家有没有过这样的经历： 学了很多但是讲不出来 知识点无法串成一个体系 感觉东西越学越多 我也有过相似的经历，并且为此困惑了很长的时间，直到看到一句话： 最好的学习方式就是“输出”。 的确，从别人那“听”来的知识，都是别人的，只有从自己这“输出”出去的知识才是自己的。这“输出”的过程，等于是强迫大脑对自己的知识进行系统化联接的过程，是一个对自己知识在加工的过程。所以，当我们做过一次演讲、发表一篇文章后，总感觉自己对相关的知识的理解更深了。 怎么“输出”知识首先要明确的是，“输出”的知识不一定要给到某一个具体的对象，而是通过这个过程对自己的思路和知识储备有一个归纳整理的过程，因此，“输出”的方法就有很多啦。 1.写博客。随便找个地儿，有什么心得感悟，就记录下来，就像写日记一样（其实写日记真的是一个很好的习惯）。等到过一段时间再会过头来印证，彼时的理解对不对？2.现场分享。不要害怕和大家交流自己的观点，世上不会有相同的两片叶子，大胆说出自己的想法，不仅使别人得到启发，更重要的是自己在说的过程中对思维进行了提炼。3.对着镜子讲话。这是我发现的一个秘诀。有时候可能对自己不是很有自信，或是想法还没有很成熟的时候，就对镜子对着自己说，尽可能的把自己的想法描述得很清晰。当你能很清晰很有层次的描述某个问题的时候，说明你已经理解了它；当你能用三言两语把它讲清楚的时候，你已经掌握了它；当你无论碰到谁都能用合适的语言在短时间内使其窥探到问题的全貌的时候，你已经驾驭了它。 现在网络很发达，要做什么分享也很方便，无数的平台无数的渠道，在互联网城邦中，做一个有分享精神的人，利人利己利国利民。：D","tags":[{"name":"IQ","slug":"IQ","permalink":"http://xrazor.net/tags/IQ/"}]}]